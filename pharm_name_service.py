
import logging
import torch
from transformers import T5Tokenizer, T5ForConditionalGeneration
from cache_nlp_results import PharmaNameCache  # –º–æ–¥–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∫–ª–∞—Å—Å –∏–∑ —à–∞–≥–∞ 1
import time
from embedding_utils import EmbeddingComparator


DEFAULT_SIMILARITY_THRESHOLD = 0.8



class PharmaNameService:
    def __init__(self, config, embedding_comparator: EmbeddingComparator):
        
        AUTOSAVE_INTERVAL = config.get("faiss_autosave_interval", 300)  # –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5 –º–∏–Ω—É—Ç
        self.cache = PharmaNameCache(embedding_comparator, autosave_interval=AUTOSAVE_INTERVAL)
        MODEL_PATH = config.get("model_path", "Goodwill333/T5-NER-Pharm-RU")
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")

        self.tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)
        self.model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH,  cache_dir=".").to(self.device)
        logging.info("–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

    def predict(self, product: str) -> str:
        input_text = "\n".join([
            "–ó–∞–¥–∞–Ω–∏–µ: –ò–∑–≤–ª–µ–∫–∏ —á–∞—Å—Ç–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞ –∏–ª–∏ —Ç–æ–≤–∞—Ä–∞ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è.",
            f"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: {product}"
        ])
        inputs = self.tokenizer(input_text, return_tensors="pt", truncation=True).to(self.device)
        start_time = time.time()
        outputs = self.model.generate(**inputs, max_length=256, num_beams=2, early_stopping=True)
        end_time = time.time()
        generation_time = end_time - start_time
        message = f"‚è±Ô∏è –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {generation_time:.2f} —Å–µ–∫—É–Ω–¥"
        logging.info(message)
        print(message)
        
        decoded = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        message2 = f"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: {product}\nüîç decoded: {decoded}"
        logging.info(message2)
        print(message2)
        return decoded.replace("\n", " ").strip()


    def predict_batch(self, products: list[str]) -> list[str]:
        input_texts = [
            "\n".join([
                "–ó–∞–¥–∞–Ω–∏–µ: –ò–∑–≤–ª–µ–∫–∏ —á–∞—Å—Ç–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞ –∏–ª–∏ —Ç–æ–≤–∞—Ä–∞ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è.",
                f"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: {product}"
            ])
            for product in products
        ]
        
        inputs = self.tokenizer(
            input_texts, 
            padding=True, 
            truncation=True, 
            return_tensors="pt",
            max_length=256
        ).to(self.device)
        
        start_time = time.time()
        
        outputs = self.model.generate(
            **inputs, 
            max_length=256, 
            num_beams=2, 
            early_stopping=True
        )
        end_time = time.time()
        generation_time = end_time - start_time
        message = f"‚è±Ô∏è –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ –±–∞—Ç—á—É —Ä–∞–∑–º–µ—Ä–æ–º {len(products)}: {generation_time:.2f} —Å–µ–∫—É–Ω–¥"
        logging.info(message)
        print(message)
       
        result = [
            self.tokenizer.decode(output, skip_special_tokens=True)
            .replace("\n", " ")
            .strip()
            for output in outputs
        ]
        return result


    def process_name(self, name, use_cache=True, similarity_threshold=DEFAULT_SIMILARITY_THRESHOLD):
        if use_cache:
            cached_result = self.cache.query_cache(name, similarity_threshold=similarity_threshold)
            if cached_result is not None:
                parsed, cached_name, dist = cached_result
                print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–ª—è: {name} (–Ω–∞–π–¥–µ–Ω–æ: {cached_name}, dist={dist:.3f})")
                return {"parsed": parsed, "cached_name": cached_name, "from_cache": True, "similarity": float(dist)}

        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å—é T5: {name}")
        result = self.predict(name)
        self.cache.add_to_cache(name, result)
        return {"parsed": result, "cached_name": None, "from_cache": False, "similarity": None}
    
    def process_names_batch(self, products, use_cache=True, similarity_threshold=DEFAULT_SIMILARITY_THRESHOLD):
        uncached_products = []
        cached_results = []
        for product in products:
            cached = self.cache.query_cache(product, similarity_threshold=similarity_threshold)
            if cached is not None:
                parsed, cached_name, dist = cached
                cached_results.append({"parsed": parsed, "cached_name": cached_name, "from_cache": True, "similarity": float(dist)})
            else:
                uncached_products.append(product)
        # –î–ª—è –≤—Å–µ—Ö –Ω–µ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –≤ –∫—ç—à–µ ‚Äî –±–∞—Ç—á–µ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        if uncached_products:
            batch_results = self.predict_batch(uncached_products)
            for product, result in zip(uncached_products, batch_results):
                self.cache.add_to_cache(product, result)
                cached_results.append({"parsed": result, "cached_name": None, "from_cache": False, "similarity": None})
        return {"results": cached_results}