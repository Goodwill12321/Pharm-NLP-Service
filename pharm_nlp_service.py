from fastapi import FastAPI
from pydantic import BaseModel
from cache_nlp_results import PharmaNameCache
import logging
import time
from transformers import T5Tokenizer, T5ForConditionalGeneration

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ —Ñ–∞–π–ª
logging.basicConfig(
    filename='service.log',
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

app = FastAPI()

#MODEL_PATH = "./t5-med-ner"  # –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç—å –∫ –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏
MODEL_PATH = "Goodwill333/T5-NER-Pharm-RU"

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –ø—Ä–∏ —Å—Ç–∞—Ä—Ç–µ
logging.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞...")
import torch

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(device)

tokenizer = T5Tokenizer.from_pretrained(MODEL_PATH)
model = T5ForConditionalGeneration.from_pretrained(MODEL_PATH,  cache_dir=".").to(device)
logging.info("–ú–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∑–∞–≥—Ä—É–∂–µ–Ω—ã")

class ProductRequest(BaseModel):
    product: str

def predict(product: str) -> str:
    input_text = "\n".join([
        "–ó–∞–¥–∞–Ω–∏–µ: –ò–∑–≤–ª–µ–∫–∏ —á–∞—Å—Ç–∏ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è –ª–µ–∫–∞—Ä—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–ø–∞—Ä–∞—Ç–∞ –∏–ª–∏ —Ç–æ–≤–∞—Ä–∞ —Ñ–∞—Ä–º–∞—Ü–µ–≤—Ç–∏—á–µ—Å–∫–æ–≥–æ –Ω–∞–∑–Ω–∞—á–µ–Ω–∏—è.",
        f"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: {product}"
    ])
    inputs = tokenizer(input_text, return_tensors="pt", truncation=True)
    start_time = time.time()
    outputs = model.generate(**inputs, max_length=256, num_beams=2, early_stopping=True)
    end_time = time.time()
    generation_time = end_time - start_time
    message = f"‚è±Ô∏è –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {generation_time:.2f} —Å–µ–∫—É–Ω–¥"
    logging.info(message)
    print(message)
    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    message2 = f"–ù–∞–∏–º–µ–Ω–æ–≤–∞–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞: {product}\nüîç decoded: {decoded}"
    logging.info(message2)
    print(message2)
    return decoded.replace("\n", " ").strip()


@app.post("/predict")
async def predict_endpoint(request: ProductRequest):
    result = service.process_name(request.product)
    return {"result": result}




# –û—Å–Ω–æ–≤–Ω–æ–π —Å–µ—Ä–≤–∏—Å —Å –∫—ç—à–µ–º
class PharmaNameService:
    def __init__(self):
        self.cache = PharmaNameCache()

    def process_name(self, name):
        # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞
        cached_result = self.cache.query_cache(name)
        if cached_result is not None:
            print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –¥–ª—è: {name}")
            return cached_result
        
        # –ï—Å–ª–∏ –Ω–µ—Ç –ø–æ–¥—Ö–æ–¥—è—â–µ–≥–æ –∫—ç—à–∞ ‚Äî –≤—ã–∑—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å T5
        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –º–æ–¥–µ–ª—å—é T5: {name}")
        result = predict(name)
        
        # –î–æ–±–∞–≤–ª—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à
        self.cache.add_to_cache(name, result)
        return result





import uvicorn

if __name__ == "__main__":
    service = PharmaNameService()
    names = [
        "–ü–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª 500–º–≥",
        "–ü–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª 650–º–≥",
        "–ò–±—É–ø—Ä–æ—Ñ–µ–Ω 200–º–≥",
        "–ü–∞—Ä–∞—Ü–µ—Ç–∞–º–æ–ª 500–º–≥ —Ç–∞–±–ª–µ—Ç–∫–∏"
    ]
    
    for n in names:
        parts = service.process_name(n)
        print(parts)

    uvicorn.run("pharm_nlp_service:app", host="127.0.0.1", port=8000, reload=True)
